{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Lane Finding Project\n",
    "\n",
    "The goals / steps of this project are the following:\n",
    "\n",
    "* Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.\n",
    "* Apply a distortion correction to raw images.\n",
    "* Use color transforms, gradients, etc., to create a thresholded binary image.\n",
    "* Apply a perspective transform to rectify binary image (\"birds-eye view\").\n",
    "* Detect lane pixels and fit to find the lane boundary.\n",
    "* Determine the curvature of the lane and vehicle position with respect to center.\n",
    "* Warp the detected lane boundaries back onto the original image.\n",
    "* Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position.\n",
    "\n",
    "---\n",
    "## First, I'll compute the camera calibration using chessboard images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run only once, to solve the conflict with ROS\n",
    "import sys\n",
    "sys.path.remove('/opt/ros/kinetic/lib/python2.7/dist-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "%matplotlib qt\n",
    "\n",
    "# prepare object points\n",
    "nx = 9 # TODO: enter the number of inside corners in x\n",
    "ny = 6 # TODO: enter the number of inside corners in y\n",
    "\n",
    "# prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\n",
    "objp = np.zeros((ny*nx,3), np.float32)\n",
    "objp[:,:2] = np.mgrid[0:nx,0:ny].T.reshape(-1,2)\n",
    "\n",
    "# Arrays to store object points and image points from all the images.\n",
    "objpoints = [] # 3d points in real world space\n",
    "imgpoints = [] # 2d points in image plane.\n",
    "\n",
    "# Make a list of calibration images\n",
    "images = glob.glob('camera_cal/calibration*.jpg')\n",
    "\n",
    "# Step through the list and search for chessboard corners\n",
    "for fname in images:\n",
    "    img = cv2.imread(fname)\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Find the chessboard corners\n",
    "    ret, corners = cv2.findChessboardCorners(gray, (9,6),None)\n",
    "\n",
    "    # If found, add object points, image points\n",
    "    if ret == True:\n",
    "        objpoints.append(objp)\n",
    "        imgpoints.append(corners)\n",
    "\n",
    "        # Draw and display the corners\n",
    "        img = cv2.drawChessboardCorners(img, (9,6), corners, ret)\n",
    "        cv2.imshow('img',img)\n",
    "        cv2.waitKey(10)\n",
    "\n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And so on and so forth..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Apply a distortion correction to raw images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate the distortion function\n",
    "# Write a function that takes an image, object points, and image points\n",
    "# performs the camera calibration, image distortion correction and \n",
    "# returns the undistorted image\n",
    "def cal_undistort(img, objpoints, imgpoints):\n",
    "    # Use cv2.calibrateCamera() and cv2.undistort()\n",
    "    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, img.shape[1:], None, None)\n",
    "    undist = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    return undist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Use color transforms, gradients, etc., to create a thresholded binary image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def gradients_threshold(grad_channel, ksize=15, mag_thresh=(30, 255), dir_thresh=(0.7, 1.3)):\n",
    "    \n",
    "    # Apply x or y gradient with the OpenCV Sobel() function\n",
    "    # and take the absolute value\n",
    "    abs_sobel_x = np.absolute(cv2.Sobel(grad_channel, cv2.CV_64F, 1, 0))\n",
    "    abs_sobel_y = np.absolute(cv2.Sobel(grad_channel, cv2.CV_64F, 0, 1))\n",
    "    \n",
    "    # Calculate the gradient magnitude\n",
    "    gradmag = np.sqrt(abs_sobel_x**2 + abs_sobel_y**2)\n",
    "    # Rescale to 8 bit\n",
    "    scale_factor = np.max(gradmag)/255 \n",
    "    gradmag = (gradmag/scale_factor).astype(np.uint8) \n",
    "    # Create a binary image of ones where threshold is met, zeros otherwise\n",
    "    mag_binary = np.zeros_like(gradmag)\n",
    "    mag_binary[(gradmag >= mag_thresh[0]) & (gradmag <= mag_thresh[1])] = 1\n",
    "    \n",
    "    # Take the absolute value of the gradient direction, \n",
    "    # apply a threshold, and create a binary image result\n",
    "    absgraddir = np.arctan2(abs_sobel_x, abs_sobel_y)\n",
    "    dir_binary =  np.zeros_like(absgraddir)\n",
    "    dir_binary[(absgraddir >= dir_thresh[0]) & (absgraddir <= dir_thresh[1])] = 1\n",
    "\n",
    "    # Combine the pixels where: \n",
    "    #     the gradient magnitude and direction are both within their threshold values\n",
    "    combined = np.zeros_like(mag_binary)\n",
    "    combined[(mag_binary == 1) & (dir_binary == 1)] = 1\n",
    "    \n",
    "    '''\n",
    "    # Plotting images\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "    f.tight_layout()\n",
    "    ax1.set_title('magnitude', fontsize=50)\n",
    "    ax1.imshow(mag_binary, cmap='gray')\n",
    "    ax2.set_title('direction', fontsize=50)\n",
    "    ax2.imshow(dir_binary, cmap='gray')\n",
    "    plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)\n",
    "    plt.show()\n",
    "    '''   \n",
    "    \n",
    "    return combined\n",
    "\n",
    "\n",
    "# Gradients & Color threshold combined function\n",
    "def combined_threshold(img, ksize=15, mag_thresh=(30, 255), dir_thresh=(0.7, 1.3), l_thresh=(120, 255), s_thresh=(170, 255)):\n",
    "    # Choose a Sobel kernel size\n",
    "    # Choose a larger odd number to smooth gradient measurements\n",
    "    \n",
    "    # Convert to HLS color space and separate the L and S channel\n",
    "    l_channel = img[:,:,0]\n",
    "    hls = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n",
    "    # l_channel = hls[:,:,1]\n",
    "    s_channel = hls[:,:,2]\n",
    "    \n",
    "    # Threshold l channel\n",
    "    scale_factor = np.max(l_channel)/255 \n",
    "    l_channel = (l_channel/scale_factor).astype(np.uint8) \n",
    "    l_binary = np.zeros_like(l_channel)\n",
    "    l_binary[(l_channel >= l_thresh[0]) & (l_channel <= l_thresh[1])] = 1\n",
    "    \n",
    "    # Threshold s channel\n",
    "    scale_factor = np.max(s_channel)/255\n",
    "    s_channel = (s_channel/scale_factor).astype(np.uint8)\n",
    "    s_binary = np.zeros_like(s_channel)\n",
    "    s_binary[(s_channel >= s_thresh[0]) & (s_channel <= s_thresh[1])] = 1\n",
    "    \n",
    "    # Select l channel to apply gradients thresholding\n",
    "    l_grad_threshed = gradients_threshold(l_binary, ksize, mag_thresh, dir_thresh) # x_thresh, y_thresh,\n",
    "    # Select s channel to apply gradients thresholding\n",
    "    s_grad_threshed = gradients_threshold(s_binary, ksize, mag_thresh, dir_thresh) # x_thresh, y_thresh,\n",
    "    \n",
    "    # Stack each channel\n",
    "    combined_ls = np.zeros_like(s_channel)\n",
    "    combined_ls[(s_grad_threshed == 1) | (l_grad_threshed == 1)] = 1\n",
    "    combined_color = np.dstack(( np.zeros_like(l_grad_threshed), l_grad_threshed, s_grad_threshed )) * 255\n",
    "    \n",
    "    '''\n",
    "    # Plotting images\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "    f.tight_layout()\n",
    "    ax1.set_title('L', fontsize=50)\n",
    "    ax1.imshow(l_binary, cmap='gray')\n",
    "    ax2.set_title('S', fontsize=50)\n",
    "    ax2.imshow(s_binary, cmap='gray')\n",
    "    plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)\n",
    "    plt.show()\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    return combined_ls, combined_color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Apply a perspective transform to rectify binary image (\"birds-eye view\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def perspective_transform(img, M, imshape):\n",
    "    \n",
    "    transformed = cv2.warpPerspective(img, M, imshape)\n",
    "    \n",
    "    return transformed\n",
    "\n",
    "\n",
    "def calculate_transform_matrix(imshape, left, right, top, bot, top_left, new_left):\n",
    "    \n",
    "    src_left_top = (top_left, top)\n",
    "    src_right_top = (imshape[0] - top_left, top)\n",
    "    src_right_bot = (right, bot)\n",
    "    src_left_bot = (left, bot)\n",
    "    src = np.float32([[src_left_top, src_right_top, src_right_bot, src_left_bot]])\n",
    "    new_right = imshape[0] - new_left\n",
    "    dst_left_top = (new_left, 0)\n",
    "    dst_right_top = (new_right, 0)\n",
    "    dst_right_bot = (new_right, imshape[1])\n",
    "    dst_left_bot = (new_left, imshape[1])\n",
    "    dst = np.float32([[dst_left_top, dst_right_top, dst_right_bot, dst_left_bot]])\n",
    "    \n",
    "    # calculate transform and inverse transform\n",
    "    M = cv2.getPerspectiveTransform(src, dst)\n",
    "    Minv = cv2.getPerspectiveTransform(dst, src)\n",
    "    \n",
    "    return M, Minv\n",
    "\n",
    "\n",
    "def calculate_center_margin(imshape, new_left):\n",
    "    \n",
    "    center_margin = int( 0.5*imshape[0] - new_left )\n",
    "    \n",
    "    return center_margin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Detect lane pixels and fit to find the lane boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_lane_pixels(binary_warped):\n",
    "    # Take a histogram of the bottom half of the image\n",
    "    histogram = np.sum(binary_warped[binary_warped.shape[0]//2:,:], axis=0)\n",
    "    # Create an output image to draw on and visualize the result\n",
    "    out_img = np.dstack((binary_warped, binary_warped, binary_warped))\n",
    "    # Find the peak of the left and right halves of the histogram\n",
    "    # These will be the starting point for the left and right lines\n",
    "    midpoint = np.int(histogram.shape[0]//2)\n",
    "    leftx_base = np.argmax(histogram[:midpoint])\n",
    "    rightx_base = np.argmax(histogram[midpoint:]) + midpoint\n",
    "\n",
    "    # HYPERPARAMETERS\n",
    "    # Choose the number of sliding windows\n",
    "    nwindows = 9\n",
    "    # Set the width of the windows +/- margin\n",
    "    margin = 100\n",
    "    # Set minimum number of pixels found to recenter window\n",
    "    minpix = 50\n",
    "\n",
    "    # Set height of windows - based on nwindows above and image shape\n",
    "    window_height = np.int(binary_warped.shape[0]//nwindows)\n",
    "    # Identify the x and y positions of all nonzero pixels in the image\n",
    "    nonzero = binary_warped.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "    # Current positions to be updated later for each window in nwindows\n",
    "    leftx_current = leftx_base\n",
    "    rightx_current = rightx_base\n",
    "\n",
    "    # Create empty lists to receive left and right lane pixel indices\n",
    "    left_lane_inds = []\n",
    "    right_lane_inds = []\n",
    "\n",
    "    # Step through the windows one by one\n",
    "    for window in range(nwindows):\n",
    "        # Identify window boundaries in x and y (and right and left)\n",
    "        win_y_low = binary_warped.shape[0] - (window+1)*window_height\n",
    "        win_y_high = binary_warped.shape[0] - window*window_height\n",
    "        win_xleft_low = leftx_current - margin\n",
    "        win_xleft_high = leftx_current + margin\n",
    "        win_xright_low = rightx_current - margin\n",
    "        win_xright_high = rightx_current + margin\n",
    "        \n",
    "        # Draw the windows on the visualization image\n",
    "        # cv2.rectangle(out_img,(win_xleft_low,win_y_low),\n",
    "        # (win_xleft_high,win_y_high),(0,255,0), 2) \n",
    "        # cv2.rectangle(out_img,(win_xright_low,win_y_low),\n",
    "        # (win_xright_high,win_y_high),(0,255,0), 2) \n",
    "        \n",
    "        # Identify the nonzero pixels in x and y within the window #\n",
    "        good_left_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & \n",
    "        (nonzerox >= win_xleft_low) &  (nonzerox < win_xleft_high)).nonzero()[0]\n",
    "        good_right_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & \n",
    "        (nonzerox >= win_xright_low) &  (nonzerox < win_xright_high)).nonzero()[0]\n",
    "        \n",
    "        # Append these indices to the lists\n",
    "        left_lane_inds.append(good_left_inds)\n",
    "        right_lane_inds.append(good_right_inds)\n",
    "        \n",
    "        # If you found > minpix pixels, recenter next window on their mean position\n",
    "        if len(good_left_inds) > minpix:\n",
    "            leftx_current = np.int(np.mean(nonzerox[good_left_inds]))\n",
    "        if len(good_right_inds) > minpix:        \n",
    "            rightx_current = np.int(np.mean(nonzerox[good_right_inds]))\n",
    "\n",
    "    # Concatenate the arrays of indices (previously was a list of lists of pixels)\n",
    "    try:\n",
    "        left_lane_inds = np.concatenate(left_lane_inds)\n",
    "        right_lane_inds = np.concatenate(right_lane_inds)\n",
    "    except ValueError:\n",
    "        # Avoids an error if the above is not implemented fully\n",
    "        pass\n",
    "\n",
    "    # Extract left and right line pixel positions\n",
    "    leftx = nonzerox[left_lane_inds]\n",
    "    lefty = nonzeroy[left_lane_inds] \n",
    "    rightx = nonzerox[right_lane_inds]\n",
    "    righty = nonzeroy[right_lane_inds]\n",
    "\n",
    "    return leftx, lefty, rightx, righty, out_img\n",
    "\n",
    "\n",
    "def fit_polynomial(binary_warped, center_margin):\n",
    "    # Find our lane pixels first\n",
    "    leftx, lefty, rightx, righty, out_img = find_lane_pixels(binary_warped)\n",
    "\n",
    "    # Fit a second order polynomial to each using `np.polyfit`\n",
    "    left_fit = np.polyfit(lefty, leftx, 2)\n",
    "    right_fit = np.polyfit(righty, rightx, 2)\n",
    "\n",
    "    # Generate x and y values for plotting\n",
    "    ploty = np.linspace(0, binary_warped.shape[0]-1, binary_warped.shape[0] )\n",
    "    try:\n",
    "        left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n",
    "        right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]\n",
    "        center_fitx = (left_fitx + right_fitx)/2\n",
    "        center_fit = np.polyfit(ploty, center_fitx, 2)\n",
    "    except TypeError:\n",
    "        # Avoids an error if `left` and `right_fit` are still none or incorrect\n",
    "        print('The function failed to fit a line!')\n",
    "        left_fitx = 1*ploty**2 + 1*ploty\n",
    "        right_fitx = 1*ploty**2 + 1*ploty\n",
    "\n",
    "    ## Visualization ##\n",
    "    # Colors in the left and right lane regions\n",
    "    # out_img[lefty, leftx] = [255, 0, 0]\n",
    "    # out_img[righty, rightx] = [0, 0, 255]\n",
    "\n",
    "    # Create fitted lane points\n",
    "    left_fit_pts = np.vstack((left_fitx, ploty)).astype(np.int32).T\n",
    "    right_fit_pts = np.vstack((right_fitx, ploty)).astype(np.int32).T\n",
    "    center_fit_pts = np.vstack((center_fitx, ploty)).astype(np.int32).T\n",
    "    \n",
    "    # Draw the fitted lane points\n",
    "    fit_thickness = int( binary_warped.shape[1]/15 )\n",
    "    cv2.polylines(out_img,  [left_fit_pts],  False,  (0, 0, 100),  fit_thickness)\n",
    "    cv2.polylines(out_img,  [right_fit_pts],  False,  (100, 0, 0),  fit_thickness)\n",
    "    cv2.polylines(out_img,  [center_fit_pts],  False,  (0, 100, 0),  2*center_margin)\n",
    "    ## End visualization steps ##\n",
    "    \n",
    "    return out_img, center_fit, left_fit, right_fit \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. b) Convolution method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def window_mask(width, height, img_ref, center,level):\n",
    "    output = np.zeros_like(img_ref)\n",
    "    output[int(img_ref.shape[0]-(level+1)*height):int(img_ref.shape[0]-level*height),max(0,int(center-width/2)):min(int(center+width/2),img_ref.shape[1])] = 1\n",
    "    return output\n",
    "\n",
    "\n",
    "def find_window_centroids(image, window_width, window_height, margin):\n",
    "    \n",
    "    window_centroids = [] # Store the (left,right) window centroid positions per level\n",
    "    window = np.ones(window_width) # Create our window template that we will use for convolutions\n",
    "    \n",
    "    # First find the two starting positions for the left and right lane by using np.sum to get the vertical image slice\n",
    "    # and then np.convolve the vertical image slice with the window template \n",
    "    \n",
    "    # Sum quarter bottom of image to get slice, could use a different ratio\n",
    "    l_sum = np.sum(image[int(3*image.shape[0]/4):,:int(image.shape[1]/2)], axis=0)\n",
    "    l_center = np.argmax(np.convolve(window,l_sum))-window_width/2\n",
    "    r_sum = np.sum(image[int(3*image.shape[0]/4):,int(image.shape[1]/2):], axis=0)\n",
    "    r_center = np.argmax(np.convolve(window,r_sum))-window_width/2+int(image.shape[1]/2)\n",
    "    \n",
    "    # Add what we found for the first layer\n",
    "    window_centroids.append((l_center,r_center))\n",
    "    \n",
    "    # Go through each layer looking for max pixel locations\n",
    "    for level in range(1,(int)(image.shape[0]/window_height)):\n",
    "\t    # convolve the window into the vertical slice of the image\n",
    "\t    image_layer = np.sum(image[int(image.shape[0]-(level+1)*window_height):int(image.shape[0]-level*window_height),:], axis=0)\n",
    "\t    conv_signal = np.convolve(window, image_layer)\n",
    "\t    # Find the best left centroid by using past left center as a reference\n",
    "\t    # Use window_width/2 as offset because convolution signal reference is at right side of window, not center of window\n",
    "\t    offset = window_width/2\n",
    "\t    l_min_index = int(max(l_center+offset-margin,0))\n",
    "\t    l_max_index = int(min(l_center+offset+margin,image.shape[1]))\n",
    "\t    l_center = np.argmax(conv_signal[l_min_index:l_max_index])+l_min_index-offset\n",
    "\t    # Find the best right centroid by using past right center as a reference\n",
    "\t    r_min_index = int(max(r_center+offset-margin,0))\n",
    "\t    r_max_index = int(min(r_center+offset+margin,image.shape[1]))\n",
    "\t    r_center = np.argmax(conv_signal[r_min_index:r_max_index])+r_min_index-offset\n",
    "\t    # Add what we found for that layer\n",
    "\t    window_centroids.append((l_center,r_center))\n",
    "\n",
    "    return window_centroids\n",
    "\n",
    "\n",
    "def convolution_window(warped, center_margin):\n",
    "    \n",
    "    # window settings\n",
    "    window_width = 50 \n",
    "    window_height = 120 # Break image into 9 vertical layers since image height is 720\n",
    "    margin = 100 # How much to slide left and right for searching\n",
    "    \n",
    "    imshape = (warped.shape[1], warped.shape[0])\n",
    "    out_img = np.dstack((warped, warped, warped))\n",
    "    \n",
    "    # window_centroids contains (l_center,r_center), from bottom\n",
    "    window_centroids = find_window_centroids(warped, window_width, window_height, margin)\n",
    "\n",
    "    # If we found any window centers\n",
    "    if len(window_centroids) > 0:\n",
    "\n",
    "        # Points used to draw all the left and right windows\n",
    "        leftx = []\n",
    "        rightx = []\n",
    "        lefty = []\n",
    "        righty = []\n",
    "        # Go through each level and draw the windows \t\n",
    "        for level in range(0,len(window_centroids)):\n",
    "            leftx.append(window_centroids[level][0])\n",
    "            rightx.append(window_centroids[level][1])\n",
    "            lefty.append(imshape[1] - level*window_height - window_height/2)\n",
    "            righty.append(imshape[1] - level*window_height - window_height/2)\n",
    "            \n",
    "        # Fit a second order polynomial to each using `np.polyfit`\n",
    "        left_fit = np.polyfit(lefty, leftx, 2)\n",
    "        right_fit = np.polyfit(righty, rightx, 2)\n",
    "\n",
    "        # Generate x and y values for plotting\n",
    "        ploty = np.linspace(0, imshape[1]-1, imshape[1] )\n",
    "        try:\n",
    "            left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n",
    "            right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]\n",
    "            center_fitx = (left_fitx + right_fitx)/2\n",
    "            center_fit = np.polyfit(ploty, center_fitx, 2)\n",
    "        except TypeError:\n",
    "            # Avoids an error if `left` and `right_fit` are still none or incorrect\n",
    "            print('The function failed to fit a line!')\n",
    "            left_fitx = 1*ploty**2 + 1*ploty\n",
    "            right_fitx = 1*ploty**2 + 1*ploty\n",
    "            \n",
    "        # Create fitted lane points\n",
    "        left_fit_pts = np.vstack((left_fitx, ploty)).astype(np.int32).T\n",
    "        right_fit_pts = np.vstack((right_fitx, ploty)).astype(np.int32).T\n",
    "        center_fit_pts = np.vstack((center_fitx, ploty)).astype(np.int32).T\n",
    "    \n",
    "        # Draw the fitted lane points\n",
    "        fit_thickness = int( imshape[0]/15 )\n",
    "        cv2.polylines(out_img,  [left_fit_pts],  False,  (0, 0, 100),  fit_thickness)\n",
    "        cv2.polylines(out_img,  [right_fit_pts],  False,  (100, 0, 0),  fit_thickness)\n",
    "        cv2.polylines(out_img,  [center_fit_pts],  False,  (0, 100, 0),  2*center_margin)\n",
    "        ## End visualization steps ## \n",
    "        \n",
    "    # If no window centers found, just display orginal road image\n",
    "    else:\n",
    "        out_img = np.array(cv2.merge((warped,warped,warped)),np.uint8)\n",
    "    \n",
    "    return out_img, center_fit, left_fit, right_fit \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once lane lines are found, only search around within a margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Search around within the margin\n",
    "def fit_poly(img_shape, leftx, lefty, rightx, righty):\n",
    "    ### TO-DO: Fit a second order polynomial to each with np.polyfit() ###\n",
    "    left_fit = np.polyfit(lefty, leftx, 2)\n",
    "    right_fit = np.polyfit(righty, rightx, 2)\n",
    "    # Generate x and y values for plotting\n",
    "    ploty = np.linspace(0, img_shape[0]-1, img_shape[0])\n",
    "    ### TO-DO: Calc both polynomials using ploty, left_fit and right_fit ###\n",
    "    left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n",
    "    right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]\n",
    "    \n",
    "    return left_fitx, right_fitx, ploty\n",
    "\n",
    "def search_around_poly(binary_warped, center_margin, left_fit=[0, 0, 0], right_fit=[0, 0, 0]):\n",
    "    # HYPERPARAMETER\n",
    "    # Choose the width of the margin around the previous polynomial to search\n",
    "    # The quiz grader expects 100 here, but feel free to tune on your own!\n",
    "    if( (left_fit != [0, 0, 0]) | (right_fit != [0, 0, 0]) ):\n",
    "        margin = 100\n",
    "    \n",
    "        # Grab activated pixels\n",
    "        nonzero = binary_warped.nonzero()\n",
    "        nonzeroy = np.array(nonzero[0])\n",
    "        nonzerox = np.array(nonzero[1])\n",
    "    \n",
    "        ### TO-DO: Set the area of search based on activated x-values ###\n",
    "        ### within the +/- margin of our polynomial function ###\n",
    "        ### Hint: consider the window areas for the similarly named variables ###\n",
    "        ### in the previous quiz, but change the windows to our new search area ###\n",
    "        left_lane_inds = ((nonzerox > (left_fit[0]*(nonzeroy**2) + left_fit[1]*nonzeroy + \n",
    "                                       left_fit[2] - margin)) & (nonzerox < (left_fit[0]*(nonzeroy**2) + \n",
    "                                                                             left_fit[1]*nonzeroy + left_fit[2] + margin)))\n",
    "        right_lane_inds = ((nonzerox > (right_fit[0]*(nonzeroy**2) + right_fit[1]*nonzeroy + \n",
    "                                        right_fit[2] - margin)) & (nonzerox < (right_fit[0]*(nonzeroy**2) + \n",
    "                                                                               right_fit[1]*nonzeroy + right_fit[2] + margin)))\n",
    "    \n",
    "        # Again, extract left and right line pixel positions\n",
    "        leftx = nonzerox[left_lane_inds]\n",
    "        lefty = nonzeroy[left_lane_inds] \n",
    "        rightx = nonzerox[right_lane_inds]\n",
    "        righty = nonzeroy[right_lane_inds]\n",
    "\n",
    "        # Fit new polynomials\n",
    "        left_fitx, right_fitx, ploty = fit_poly(binary_warped.shape, leftx, lefty, rightx, righty)\n",
    "        # Fit the left and right\n",
    "        left_fit = np.polyfit(ploty, left_fitx, 2)\n",
    "        right_fit = np.polyfit(ploty, right_fitx, 2)\n",
    "        # Fit the center curverture\n",
    "        center_fitx = (left_fitx + right_fitx)/2\n",
    "        center_fit = np.polyfit(ploty, center_fitx, 2)\n",
    "    \n",
    "    \n",
    "        ## Visualization ##\n",
    "        # Create an image to draw on and an image to show the selection window\n",
    "        out_img = np.dstack((binary_warped, binary_warped, binary_warped)) # *255\n",
    "        window_img = np.zeros_like(out_img)\n",
    "        # Color in left and right line pixels\n",
    "        # out_img[nonzeroy[left_lane_inds], nonzerox[left_lane_inds]] = [255, 0, 0]\n",
    "        # out_img[nonzeroy[right_lane_inds], nonzerox[right_lane_inds]] = [0, 0, 255]\n",
    "\n",
    "        # Generate a polygon to illustrate the search window area\n",
    "        # And recast the x and y points into usable format for cv2.fillPoly()\n",
    "        left_line_window1 = np.array([np.transpose(np.vstack([left_fitx-margin, ploty]))])\n",
    "        left_line_window2 = np.array([np.flipud(np.transpose(np.vstack([left_fitx+margin, ploty])))])\n",
    "        left_line_pts = np.hstack((left_line_window1, left_line_window2))\n",
    "        right_line_window1 = np.array([np.transpose(np.vstack([right_fitx-margin, ploty]))])\n",
    "        right_line_window2 = np.array([np.flipud(np.transpose(np.vstack([right_fitx+margin, ploty])))])\n",
    "        right_line_pts = np.hstack((right_line_window1, right_line_window2))\n",
    "    \n",
    "        # Generate a polygon to illustrate the safe driving area\n",
    "        center_line_window1 = np.array([np.transpose(np.vstack([center_fitx-center_margin, ploty]))])\n",
    "        center_line_window2 = np.array([np.flipud(np.transpose(np.vstack([center_fitx+center_margin, ploty])))])\n",
    "        center_line_pts = np.hstack((center_line_window1, center_line_window2))\n",
    "    \n",
    "    \n",
    "        # Draw the lane onto the warped blank image\n",
    "        cv2.fillPoly(window_img, np.int_([left_line_pts]), (0, 0, 100))\n",
    "        cv2.fillPoly(window_img, np.int_([right_line_pts]), (100, 0, 0))\n",
    "        result = cv2.addWeighted(out_img, 1, window_img, 0.3, 0)\n",
    "        cv2.fillPoly(window_img, np.int_([center_line_pts]), (0, 100, 0))\n",
    "        result = cv2.addWeighted(out_img, 1, window_img, 0.3, 0)\n",
    "        \n",
    "        # Plot the polynomial lines onto the image\n",
    "        # plt.plot(left_fitx, ploty, color='yellow')\n",
    "        # plt.plot(right_fitx, ploty, color='yellow')\n",
    "        # plt.plot(center_fitx, ploty, color='red')\n",
    "        ## End visualization steps ##\n",
    "        \n",
    "    else:\n",
    "        result, center_fit, left_fit, right_fit = convolution_window(binary_warped, center_margin)\n",
    "        # result, center_fit, left_fit, right_fit = fit_polynomial(binary_warped, center_margin)\n",
    "    \n",
    "    return result, center_fit, left_fit, right_fit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Determine the curvature of the lane and vehicle position with respect to center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calculate_distance2pixel(center_margin, imshape):\n",
    "    \n",
    "    # Define conversions in x and y from pixels space to meters\n",
    "    xm_per_pix = 3.7/2/center_margin # meters per pixel in x dimension\n",
    "    ym_per_pix = 37/imshape[1] # meters per pixel in y dimension\n",
    "    \n",
    "    return xm_per_pix, ym_per_pix\n",
    "\n",
    "\n",
    "def measure_curvature_real(imshape, center_margin, center_fit):\n",
    "    \n",
    "    # Calculates the curvature of polynomial functions in meters.\n",
    "    xm_per_pix, ym_per_pix = calculate_distance2pixel(center_margin, imshape)\n",
    "    \n",
    "    center_fit_cr = [center_fit[0]*xm_per_pix/ym_per_pix/ym_per_pix, \n",
    "                     center_fit[1]*xm_per_pix/ym_per_pix, center_fit[2]*xm_per_pix]\n",
    "    \n",
    "    # Define y-value where we want radius of curvature\n",
    "    ploty = np.linspace(0, imshape[1]-1, num=imshape[1])# to cover same y-range as image\n",
    "    # We'll choose the maximum y-value, corresponding to the bottom of the image\n",
    "    y_eval = np.max(ploty)\n",
    "    \n",
    "    # Calculation of R_curve (radius of curvature)\n",
    "    center_curverad = ((1 + (2*center_fit_cr[0]*y_eval*ym_per_pix + center_fit_cr[1])**2)**1.5) / np.absolute(2*center_fit_cr[0])\n",
    "    lane_center = center_fit[0]*imshape[1]**2 + center_fit[1]*imshape[1] + center_fit[2]\n",
    "    track_error = (0.5*imshape[0] - lane_center) * xm_per_pix *100 # in centimeter\n",
    "    if(track_error >= 0):\n",
    "        position_flag = \"right\"\n",
    "    else:\n",
    "        position_flag = \"left\"\n",
    "    abs_track_error = np.abs(track_error)\n",
    "    \n",
    "    return center_curverad, abs_track_error, position_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Warp the detected lane boundaries back onto the original image. \n",
    "\n",
    "With the help of the `perspective_transform()` function in step 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def display_lane(raw_img, result, Minv, imshape):\n",
    "    \n",
    "    result_unwarped = perspective_transform(result, Minv, imshape)\n",
    "    # result_unwarped = cv2.cvtColor(result_unwarped, cv2.COLOR_BGR2RGB)\n",
    "    output_image = cv2.addWeighted(raw_img, 1, result_unwarped, 1, 0)\n",
    "    \n",
    "    return output_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def add_text(img, center_curverad, abs_track_error, position_flag):\n",
    "    center_curverad = str(np.round(center_curverad, 1))\n",
    "    abs_track_error = str(np.round(abs_track_error, 1))\n",
    "    text_1 = \"Radius of Lane = \" + center_curverad + \"m\"\n",
    "    text_2 = \"Vehicle is \" + abs_track_error + \"cm \" + position_flag + \" of center\"\n",
    "    cv2.putText(img, text_1, (200, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255, 255, 255), 2)\n",
    "    cv2.putText(img, text_2, (200, 100), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255, 255, 255), 2)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A small debug function to help show the images during the process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def debug(combined_threshed, binary_warped, result, output_image):\n",
    "    \n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "    f.tight_layout()\n",
    "    ax1.set_title('Lane Line Image', fontsize=50)\n",
    "    ax1.imshow(combined_threshed)\n",
    "    ax2.set_title('Output Image', fontsize=50)\n",
    "    ax2.imshow(binary_warped, cmap='gray')\n",
    "    plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)\n",
    "    plt.show()\n",
    "    \n",
    "    # Plotting images\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "    f.tight_layout()\n",
    "    ax1.set_title('Lane Line Image', fontsize=50)\n",
    "    ax1.imshow(result)\n",
    "    ax2.set_title('Output Image', fontsize=50)\n",
    "    ax2.imshow(output_image)\n",
    "    plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# A seperate cell to initialise left and right fit parameters\n",
    "# left_fit = np.array([2.13935315e-04, -3.77507980e-01,  4.76902175e+02])\n",
    "# right_fit = np.array([4.17622148e-04, -4.93848953e-01,  1.11806170e+03])\n",
    "left_fit = [0, 0, 0]\n",
    "right_fit = [0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Testing with the sliding windows method\n",
    "def img_pipeline(raw_img):\n",
    "    \n",
    "    '''0. Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.'''\n",
    "    '''1. Apply a distortion correction to raw images.'''\n",
    "    undistorted = cal_undistort(raw_img, objpoints, imgpoints)\n",
    "    imshape = (undistorted.shape[1], undistorted.shape[0])\n",
    "    \n",
    "    '''2. Apply a perspective transform to rectify binary image (\"birds-eye view\").'''\n",
    "    # Appoint source points and destination points\n",
    "    left = int(0.2*imshape[0])\n",
    "    right = int(0.8*imshape[0])\n",
    "    top = int(0.65*imshape[1])\n",
    "    bot = int(0.95*imshape[1])\n",
    "    top_left = int(0.445*imshape[0])\n",
    "    new_left = 200 # 200\n",
    "    # Given src and dst points, calculate the perspective transform matrix\n",
    "    M, Minv = calculate_transform_matrix(imshape, left, right, top, bot, top_left, new_left)\n",
    "    # Warp the image to top-down view\n",
    "    # binary_warped = perspective_transform(combined_threshed, M, imshape)\n",
    "    warped = perspective_transform(undistorted, M, imshape)\n",
    "    \n",
    "    '''3. Use color transforms, gradients, etc., to create a thresholded binary image.'''\n",
    "    ksize = 15\n",
    "    mag_thresh = (100, 255) # (100, 255) (150,255)\n",
    "    dir_thresh = (0.7, 1.3) # (0.7, 1.3)\n",
    "    l_thresh = (180, 255) #r (180, 255) #l (50, 130) #h (15, 100)\n",
    "    s_thresh = (50, 255) # (50, 255) (130, 255) (170, 255)\n",
    "    # Apply the threshholds\n",
    "    combined_threshed, combined_color = combined_threshold(warped, ksize, mag_thresh, dir_thresh, l_thresh, s_thresh)\n",
    "    \n",
    "    \n",
    "    '''4. Detect lane pixels and fit to find the lane boundary.'''\n",
    "    # Calculate half of the lane width in pixels\n",
    "    center_margin = calculate_center_margin(imshape, new_left)\n",
    "    \n",
    "    # a) Sliding windows method\n",
    "    result, center_fit, left_fit, right_fit = fit_polynomial(combined_threshed, center_margin)\n",
    "    # b) Convolution window method\n",
    "    # result, center_fit, left_fit, right_fit = convolution_window(binary_warped, center_margin)\n",
    "    # Polynomial fit values from the previous frame \n",
    "    # result, center_fit, left_fit, right_fit = search_around_poly(binary_warped, center_margin)\n",
    "\n",
    "    \n",
    "    '''5. Determine the curvature of the lane and vehicle position with respect to center.'''\n",
    "    # Calculate the radius of curvature in meters at the center, the error in position and left/right flag\n",
    "    center_curverad, abs_track_error, position_flag = measure_curvature_real(imshape, center_margin, center_fit)\n",
    "    \n",
    "    '''6. Warp the detected lane boundaries back onto the original image.'''\n",
    "    output_image = display_lane(raw_img, result, Minv, imshape)\n",
    "    \n",
    "    '''7. Output visual display of the lane boundaries'''\n",
    "    # and numerical estimation of lane curvature and vehicle position.\n",
    "    output_image= add_text(output_image, center_curverad, abs_track_error, position_flag)\n",
    "    \n",
    "    # Plotting images for debugging\n",
    "    # debug(combined_color, binary_warped, result, output_image)\n",
    "    \n",
    "    return output_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.57 s, sys: 31.4 ms, total: 1.6 s\n",
      "Wall time: 917 ms\n",
      "CPU times: user 2.21 s, sys: 60.2 ms, total: 2.27 s\n",
      "Wall time: 902 ms\n",
      "CPU times: user 2.25 s, sys: 36.7 ms, total: 2.28 s\n",
      "Wall time: 950 ms\n",
      "CPU times: user 2.25 s, sys: 72.2 ms, total: 2.33 s\n",
      "Wall time: 937 ms\n",
      "CPU times: user 2.31 s, sys: 60.1 ms, total: 2.37 s\n",
      "Wall time: 974 ms\n",
      "CPU times: user 2.27 s, sys: 43.7 ms, total: 2.31 s\n",
      "Wall time: 962 ms\n",
      "CPU times: user 2.19 s, sys: 28.2 ms, total: 2.22 s\n",
      "Wall time: 900 ms\n",
      "CPU times: user 2.19 s, sys: 47.8 ms, total: 2.24 s\n",
      "Wall time: 915 ms\n"
     ]
    }
   ],
   "source": [
    "for x in os.listdir(\"test_images/\"):\n",
    "    input_path = \"test_images/\" + x\n",
    "    if os.path.isdir(input_path):\n",
    "        continue\n",
    "    \n",
    "    # Read in each raw image\n",
    "    raw_img = cv2.imread(input_path) \n",
    "    \n",
    "    %time output_img = img_pipeline(raw_img)\n",
    "    \n",
    "    # Outout the image to directory \"/test_image_output\"\n",
    "    output_path = \"test_image_output/\" + x\n",
    "    if os.path.isdir(output_path):\n",
    "        continue\n",
    "    cv2.imwrite(output_path, output_img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runing on Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import everything needed to edit/save/watch video clips\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Project Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:   0%|          | 0/1260 [00:00<?, ?it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video output_videos/project_video.mp4.\n",
      "Moviepy - Writing video output_videos/project_video.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready output_videos/project_video.mp4\n",
      "CPU times: user 47min 2s, sys: 56.6 s, total: 47min 58s\n",
      "Wall time: 18min 46s\n"
     ]
    }
   ],
   "source": [
    "# Set input and output paths\n",
    "input_path_1 = \"project_video.mp4\"\n",
    "output_path_1 = 'output_videos/' + input_path_1\n",
    "\n",
    "## To speed up the testing process you may want to try your pipeline on a shorter subclip of the video\n",
    "## To do so add .subclip(start_second,end_second) to the end of the line below\n",
    "## Where start_second and end_second are integer values representing the start and end of the subclip\n",
    "## You may also uncomment the following line for a subclip of the first 5 seconds\n",
    "## clip1 = VideoFileClip(\"test_videos/solidWhiteRight.mp4\").subclip(0,5)\n",
    "clip_1 = VideoFileClip(input_path_1)\n",
    "clip_1_output = clip_1.fl_image(img_pipeline) # NOTE: this function expects color images!!\n",
    "%time clip_1_output.write_videofile(output_path_1, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"800\" height=\"450\" controls>\n",
       "  <source src=\"output_videos/project_video.mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualization\n",
    "HTML(\"\"\"\n",
    "<video width=\"800\" height=\"450\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(output_path_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Challenge Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:   0%|          | 0/485 [00:00<?, ?it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video output_videos/challenge_video.mp4.\n",
      "Moviepy - Writing video output_videos/challenge_video.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready output_videos/challenge_video.mp4\n",
      "CPU times: user 18min 5s, sys: 19.5 s, total: 18min 24s\n",
      "Wall time: 7min 15s\n"
     ]
    }
   ],
   "source": [
    "# Set input and output paths\n",
    "input_path_2 = \"challenge_video.mp4\"\n",
    "output_path_2 = 'output_videos/' + input_path_2\n",
    "## To speed up the testing process you may want to try your pipeline on a shorter subclip of the video\n",
    "## To do so add .subclip(start_second,end_second) to the end of the line below\n",
    "## Where start_second and end_second are integer values representing the start and end of the subclip\n",
    "## You may also uncomment the following line for a subclip of the first 5 seconds\n",
    "## clip1 = VideoFileClip(\"test_videos/solidWhiteRight.mp4\").subclip(0,5)\n",
    "clip = VideoFileClip(input_path_2)\n",
    "clip_output = clip.fl_image(img_pipeline) # NOTE: this function expects color images!!\n",
    "%time clip_output.write_videofile(output_path_2, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"800\" height=\"450\" controls>\n",
       "  <source src=\"output_videos/challenge_video.mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualization\n",
    "HTML(\"\"\"\n",
    "<video width=\"800\" height=\"450\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(output_path_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Harder Challenge Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:   0%|          | 0/1199 [00:00<?, ?it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video output_videos/harder_challenge_video.mp4.\n",
      "Moviepy - Writing video output_videos/harder_challenge_video.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready output_videos/harder_challenge_video.mp4\n",
      "CPU times: user 44min 48s, sys: 50.5 s, total: 45min 38s\n",
      "Wall time: 37min 58s\n"
     ]
    }
   ],
   "source": [
    "# Set input and output paths\n",
    "input_path_3 = \"harder_challenge_video.mp4\"\n",
    "output_path_3 = 'output_videos/' + input_path_3\n",
    "## To speed up the testing process you may want to try your pipeline on a shorter subclip of the video\n",
    "## To do so add .subclip(start_second,end_second) to the end of the line below\n",
    "## Where start_second and end_second are integer values representing the start and end of the subclip\n",
    "## You may also uncomment the following line for a subclip of the first 5 seconds\n",
    "## clip1 = VideoFileClip(\"test_videos/solidWhiteRight.mp4\").subclip(0,5)\n",
    "clip = VideoFileClip(input_path_3)\n",
    "clip_output = clip.fl_image(img_pipeline) # NOTE: this function expects color images!!\n",
    "%time clip_output.write_videofile(output_path_3, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"800\" height=\"450\" controls>\n",
       "  <source src=\"output_videos/harder_challenge_video.mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualization\n",
    "HTML(\"\"\"\n",
    "<video width=\"800\" height=\"450\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(output_path_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writeup\n",
    "\n",
    "---\n",
    "\n",
    "**Advanced Lane Finding Project**\n",
    "\n",
    "The goals / steps of this project are the following:\n",
    "\n",
    "* Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.\n",
    "* Apply a distortion correction to raw images.\n",
    "* Use color transforms, gradients, etc., to create a thresholded binary image.\n",
    "* Apply a perspective transform to rectify binary image (\"birds-eye view\").\n",
    "* Detect lane pixels and fit to find the lane boundary.\n",
    "* Determine the curvature of the lane and vehicle position with respect to center.\n",
    "* Warp the detected lane boundaries back onto the original image.\n",
    "* Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position.\n",
    "\n",
    "[//]: # (Image References)\n",
    "\n",
    "[image1]: ./output_images/undistort_output.jpg \"Undistort\"\n",
    "[image2]: ./output_images/undistorted.jpg \"Undistorted\"\n",
    "[image3]: ./output_images/warped.jpg \"Road Transformed\"\n",
    "[image4]: ./output_images/combined_threshed.jpg \"Binary Example\"\n",
    "[image5]: ./output_images/line_fit_result.jpg \"Fit Visual\"\n",
    "[image6]: ./output_images/output_image.jpg \"Output\"\n",
    "[video1]: ./output_videos/project_video.mp4 \"Project Video\"\n",
    "[video2]: ./output_videos/challenge_video.mp4 \"Challenge Video\"\n",
    "[video3]: ./output_videos/harder_challenge_video.mp4 \"Harder Challenge Video\"\n",
    "\n",
    "## [Rubric](https://review.udacity.com/#!/rubrics/571/view) Points\n",
    "\n",
    "### Here I will consider the rubric points individually and describe how I addressed each point in my implementation.  \n",
    "\n",
    "---\n",
    "\n",
    "### Writeup / README\n",
    "\n",
    "#### 1. Provide a Writeup / README that includes all the rubric points and how you addressed each one.  You can submit your writeup as markdown or pdf.  [Here](https://github.com/udacity/CarND-Advanced-Lane-Lines/blob/master/writeup_template.md) is a template writeup for this project you can use as a guide and a starting point.  \n",
    "\n",
    "You're reading it!\n",
    "\n",
    "### Camera Calibration\n",
    "\n",
    "#### 1. Briefly state how you computed the camera matrix and distortion coefficients. Provide an example of a distortion corrected calibration image.\n",
    "\n",
    "The code for this step is contained in the IPython notebook located in \"./P2.ipynb\", **Step 1**.  \n",
    "\n",
    "I start by preparing \"object points\", which will be the (x, y, z) coordinates of the chessboard corners in the world. Here I am assuming the chessboard is fixed on the (x, y) plane at z=0, such that the object points are the same for each calibration image.  Thus, `objp` is just a replicated array of coordinates, and `objpoints` will be appended with a copy of it every time I successfully detect all chessboard corners in a test image.  `imgpoints` will be appended with the (x, y) pixel position of each of the corners in the image plane with each successful chessboard detection.  \n",
    "\n",
    "I then used the output `objpoints` and `imgpoints` to compute the camera calibration and distortion coefficients using the `cv2.calibrateCamera()` function.  I applied this distortion correction to the calibration image using the `cv2.undistort()` function and obtained this result: \n",
    "\n",
    "![alt text][image1]\n",
    "\n",
    "### Pipeline (single images)\n",
    "\n",
    "#### 1. Provide an example of a distortion-corrected image.\n",
    "\n",
    "The code for this step is contained in the IPython notebook located in \"./P2.ipynb\", **Step 1**.  \n",
    "\n",
    "In this step, I defined a function called `cal_undistort()` to apply undistortion to the raw image. This function takes in the `img`, which is the raw testing image, and the `objpoints` `imgpoints` given in step 1. The function uses `cv2.calibrateCamera()` to calculate distortion coefficients, and `cv2.undistort()` to undistort the raw image. \n",
    "\n",
    "Here is an example of the undistorted image \n",
    "![alt text][image2]\n",
    "\n",
    "\n",
    "\n",
    "#### 2. Describe how (and identify where in your code) you used color transforms, gradients or other methods to create a thresholded binary image.  Provide an example of a binary image result.\n",
    "\n",
    "The code for this step is contained in the IPython notebook located in \"./P2.ipynb\", **Step 2**\n",
    "\n",
    "I used a combination of color and gradient thresholds to generate a binary image. \n",
    "\n",
    "First, in the function `combined_threshold()`, two color channels, R and S, were seletced for their good complementary performance in various senarios. Each of these two channels then went through magnitude & direction thresholding via `gradients_threshold()` function to generate a binary image, where logic used is \"AND\". Then, the two binary images from each channel are combined together with the \"OR\" logical operator, to make the detection algorithm more addaptive to different senarios. \n",
    "\n",
    ".  Here's an example of my output for this step.  (note: this is not actually from one of the test images)\n",
    "\n",
    "![alt text][image3]\n",
    "\n",
    "\n",
    "#### 3. Describe how (and identify where in your code) you performed a perspective transform and provide an example of a transformed image.\n",
    "\n",
    "The code for this step is contained in the IPython notebook located in \"./P2.ipynb\", **Step 3**.\n",
    "\n",
    "The code for my perspective transform includes a function called `perspective_transform()`, which takes an image `img`, transformation matrix `M`, and the `imshape`, which is the shape of the raw image (in (x,y) order) as inputs. It uses `cv2.warpPerspective()` to warp the image to a top-down view. \n",
    "\n",
    "The second funtion in this step is called `calculate_transform_matrix()`, which takes in `imshape`(the shape of the raw image (in (x,y) order), and six other inputs, which are used to define four trapezium-shape `src` points & four rectangle-shape `dst` points. The six inputs are `left`, `right`(x coordinates of the bottom left/right `src` point), `top`, `bot`(y coordinates of all the top/bottom `src` point), `top_left`(x coordinate of the top left `src` point), `new_left`(x coordinate of the top left & bottom left `dst` points). The y coordinates of top left & top right `dst` points by default are 0, while that of the bottom left & bottom right points are `imshape[1]`. The code to define the points is shown below: \n",
    "\n",
    "```python\n",
    "\n",
    "src_left_top = (top_left, top)\n",
    "src_right_top = (imshape[0] - top_left, top)\n",
    "src_right_bot = (right, bot)\n",
    "src_left_bot = (left, bot)\n",
    "src = np.float32([[src_left_top, src_right_top, src_right_bot, src_left_bot]])\n",
    "new_right = imshape[0] - new_left\n",
    "dst_left_top = (new_left, 0)\n",
    "dst_right_top = (new_right, 0)\n",
    "dst_right_bot = (new_right, imshape[1])\n",
    "dst_left_bot = (new_left, imshape[1])\n",
    "dst = np.float32([[dst_left_top, dst_right_top, dst_right_bot, dst_left_bot]])\n",
    "\n",
    "```\n",
    "\n",
    "This resulted in the following source and destination points:\n",
    "\n",
    "| Source        | Destination   | \n",
    "|:-------------:|:-------------:| \n",
    "| 256, 684      | 200, 720      | \n",
    "| 570, 468      | 200, 0        |\n",
    "| 710, 468      | 1080, 0       |\n",
    "| 1024, 684     | 1080, 720     |\n",
    "\n",
    "This function returns the transformation matrix `M`, and the inverse transformation matrix `Minv`.\n",
    "\n",
    "Here is an example of the warped lane image\n",
    "![alt text][image4]\n",
    "\n",
    "In addition, to facilitate the visualization of the drivable area, a function called `calculate_center_margin` was created. It takes in the shape of the image (`imshape`), the x coordinate of the bottom left `dst` point(`new_left`), and calculate the \"half of the lane width in pixels\". \n",
    "\n",
    "#### 4. Describe how (and identify where in your code) you identified lane-line pixels and fit their positions with a polynomial?\n",
    "\n",
    "The code for this step is contained in the IPython notebook located in \"./P2.ipynb\", **Step 4**\n",
    "\n",
    "In this step, I implemented both the \"sliding window\" method and the \"convolution window\" method. \n",
    "\n",
    "In 4.a) The function `find_lane_pixels()` first creates a histogram for the bottom quarter of the warped binary image `binary_warped`, find the left and right peaks and uses them as starting points for left and right windows. The windows at each level sliding left and right within the `margin` and stop when the number of pixels reaches `minpix`. Then, the function retruns the x,y coordinates of the left and right window centroids(`leftx`, `lefty`, `rightx`, `righty`), together with the ploted image(`out_img`) to `fit_polynomial()` to generate the left and right fit lane points and two polynomials describing them(`left_fit`, `right_fit`). I used the average of these left and right fit lane points to simulate some center fit lane points and a polynomial called `center_fit`. Finally, the function draws the left and right polynomials, and the center polynomial, with a thickness equals to `2*center_margin`, and returns the final output image `out_img` and the three polynomials. \n",
    "\n",
    "Here is an example output image from this function:\n",
    "\n",
    "![alt text][image5]\n",
    "\n",
    "In 4.b) The function `convolution_window()` implements the \"convolution window\" method, with the help of the other two functions: `window_mask()`, and `find_window_centroids()`. Genreal, this method is simmilar the \"sliding window\" method described in 4.a). Only in this time, the criteria for sliding windows changed to maximising the number of hot pixels in the window, instead of only reaching a fixed threshold value. \n",
    "\n",
    "In 4.c) Once the lane lines have been found, the function `search_around_poly()` will take over, to continue searching for lane lines around the previous results, with the help of function `fit_poly()`.\n",
    "\n",
    "#### 5. Describe how (and identify where in your code) you calculated the radius of curvature of the lane and the position of the vehicle with respect to center.\n",
    "\n",
    "The code for this step is contained in the IPython notebook located in \"./P2.ipynb\", **Step 5**\n",
    "\n",
    "Function`calculate_distance2pixel()` was creataed to transform the relationship between the warped image pixels and the real distance they represent in real world. While `measure_curvature_real()` is the main function that takes in the shape of the image(`imshape`), the `center_margin`, and the polynomial that describes the center lane(`cneter_fit`). The output of this function are the radius of the lane curveture(`center_curverad`), the absolute distance bwtween the camera center and the lane center(`abs_track_error`), and whether the vehicle is on the left or right side of the road(`position_flag`). \n",
    "\n",
    "#### 6. Provide an example image of your result plotted back down onto the road such that the lane area is identified clearly.\n",
    "\n",
    "The code for this step is contained in the IPython notebook located in \"./P2.ipynb\", **Step 6**\n",
    "\n",
    "With the help of the function `perspective_transform()` in step 3, function `display_lane()` in this step takes in the undistored image(`raw_img`), the warped lane detection result image from step 4(`result`), the inverse transformation matrix `Minv`, the shape of the image `imshape`, and transform the warped lane image to the original view. Then, it uses `cv2.addWeighted()` function to combine stack the lane image and the undistorted original image.\n",
    "\n",
    "The code for this step is contained in the IPython notebook located in \"./P2.ipynb\", **Step 7**\n",
    "\n",
    "Function `add_text()` takes in the image from the previous step(`img`), `center_curverad`,`abs_track_error`, and `position_flag`, and display these values onto the `img`. \n",
    "\n",
    "Here is an example of my result on a test image:\n",
    "\n",
    "![alt text][image6]\n",
    "\n",
    "---\n",
    "\n",
    "### Pipeline (video)\n",
    "\n",
    "#### 1. Provide a link to your final video output.  Your pipeline should perform reasonably well on the entire project video (wobbly lines are ok but no catastrophic failures that would cause the car to drive off the road!).\n",
    "\n",
    "Here's a [link to my video result](./output_videos/project_video.mp4)\n",
    "\n",
    "[video1]: ./output_videos/project_video.mp4 \"Project Video\"\n",
    "[video2]: ./output_videos/challenge_video.mp4 \"Challenge Video\"\n",
    "[video3]: ./output_videos/harder_challenge_video.mp4 \"Harder Challenge Video\"\n",
    "\n",
    "---\n",
    "\n",
    "### Discussion\n",
    "\n",
    "#### 1. Briefly discuss any problems / issues you faced in your implementation of this project.  Where will your pipeline likely fail?  What could you do to make it more robust?\n",
    "\n",
    "Here I'll talk about the approach I took, what techniques I used, what worked and why, where the pipeline might fail and how I might improve it if I were going to pursue this project further.  \n",
    "\n",
    "**Known Issues**\n",
    "\n",
    "This algorithm is likely to fail when\n",
    "\n",
    "1. Failure due to the sliding window/convolutional window\n",
    "    \n",
    "    a. The lane lines are not continuous\n",
    "    \n",
    "    b. The curveture of the lane is too big\n",
    "    \n",
    "    c. The curveture of the lane go out of FOV\n",
    "    \n",
    "    d. Driving uphill/downhill\n",
    "    \n",
    "    \n",
    "2. Failure due to the color thresholding\n",
    "    \n",
    "    a. The road color changes/shadow\n",
    "    \n",
    "    b. There are similar patterns besides the lane lines (barriers, walls, longitudinal gaps)\n",
    "    \n",
    "    c. Under extreme low/high light condition\n",
    "\n",
    "\n",
    "**Possible Solutions**\n",
    "\n",
    "1. Adjust the window settings/perspective transform settings\n",
    "    \n",
    "    a. The lane lines are not continuous\n",
    "        i) Increase the height of the window \n",
    "        ii) When no/not enough hot pixels found in one window, continue the trend of the previous windows.\n",
    "        \n",
    "    b. The curveture of the lane is too big \n",
    "        i) Decrease the height of the window\n",
    "        ii) Increase the width of the window\n",
    "        iii) Rotatable window that can adapt the lane direction maybe?\n",
    "        \n",
    "    c. The curveture of the lane go out of FOV \n",
    "        i) Increase the camera FOV \n",
    "        ii) When no/not enough hot pixels found in one window, continue the trend of the previous windows.\n",
    "        \n",
    "    d. Driving uphill/downhill \n",
    "        i) Dynamic warpping aera\n",
    "        ii) Dynamic region of interest\n",
    "        iii) Prediction of the lane direction behind objects/obstacles\n",
    "\n",
    "2. Adjust the color/gradient thresholding\n",
    "    \n",
    "    a. The road color changes/shadow \n",
    "        i) Color filter\n",
    "        ii) Tune the S channel\n",
    "        \n",
    "    b. There are similar patterns besides the lane lines (barriers, walls, longitudinal gaps) \n",
    "        i) Color filter\n",
    "        ii) Check if the left and right lane lines are parallel\n",
    "        \n",
    "    c. Under extreme low/high light condition \n",
    "        i) Dynamically adjust the lightness\n",
    "        ii) Tune the S channel or try other color channels\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3] *",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
